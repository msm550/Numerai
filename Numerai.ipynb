{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ranking train data on how similar to test data they are, and having a good validation dataset ( first suggested by https://github.com/zygmuntz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# fold 1\n",
      "# AUC: 78.73%\n",
      "\n",
      "# fold 2\n",
      "# AUC: 78.53%\n",
      "\n",
      "# fold 3\n",
      "# AUC: 79.92%\n",
      "\n",
      "# fold 4\n",
      "# AUC: 78.84%\n",
      "\n",
      "# fold 5\n",
      "# AUC: 79.22%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\"train a classifier to distinguish between train and test\"\n",
    "\"save train examples in order of similarity to test (ascending)\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import cross_validation as CV\n",
    "from sklearn.ensemble import RandomForestClassifier as RF\n",
    "from sklearn.metrics import roc_auc_score as AUC\n",
    "\n",
    "train_file = 'numerai_training_data.csv'\n",
    "test_file = 'numerai_tournament_data.csv'\n",
    "output_file = 'train_sorted.csv'\n",
    "\n",
    "\n",
    "\n",
    "train = pd.read_csv( train_file )\n",
    "test = pd.read_csv( test_file )\n",
    "\n",
    "test.drop( 't_id', axis = 1, inplace = True )\n",
    "test['target'] = 0 # dummy for preserving column order when concatenating\n",
    "\n",
    "train['is_test'] = 0\n",
    "test['is_test'] = 1\n",
    "\n",
    "orig_train = train.copy()\n",
    "assert( np.all( orig_train.columns == test.columns ))\n",
    "\n",
    "train = pd.concat(( orig_train, test ))\n",
    "train.reset_index( inplace = True, drop = True )\n",
    "\n",
    "x = train.drop( [ 'is_test', 'target' ], axis = 1 )\n",
    "y = train.is_test\n",
    "\n",
    "\n",
    "n_estimators = 200\n",
    "clf = RF( n_estimators = n_estimators, n_jobs = -1 )\n",
    "\n",
    "predictions = np.zeros( y.shape )\n",
    "\n",
    "cv = CV.StratifiedKFold( y, n_folds = 5, shuffle = True, random_state = 10000 )\n",
    "\n",
    "for f, ( train_i, test_i ) in enumerate( cv ):\n",
    "\n",
    "    print(\"# fold {}\".format( f + 1 ))\n",
    "\n",
    "    x_train = x.iloc[train_i]\n",
    "    x_test = x.iloc[test_i]\n",
    "    y_train = y.iloc[train_i]\n",
    "    y_test = y.iloc[test_i]\n",
    "    \n",
    "    clf.fit( x_train, y_train )\t\n",
    "\n",
    "    p = clf.predict_proba( x_test )[:,1]\n",
    "\n",
    "    auc = AUC( y_test, p )\n",
    "    print(\"# AUC: {:.2%}\\n\".format( auc ))\n",
    "    predictions[ test_i ] = p\n",
    "\n",
    "\n",
    "train['p'] = predictions\n",
    "\n",
    "i = predictions.argsort()\n",
    "train_sorted = train.iloc[i]\n",
    "\n",
    "\n",
    "train_sorted = train_sorted.loc[ train_sorted.is_test == 0 ]\n",
    "assert( train_sorted.target.sum() == orig_train.target.sum())\n",
    "\n",
    "train_sorted.drop( 'is_test', axis = 1, inplace = True )\n",
    "train_sorted.to_csv( output_file, index = False )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Gridsearch to choose the best C parameter of Logistic regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 50.32%, log loss: 69.43% \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression as LR\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "\n",
    "\n",
    "train_file = 'train_sorted.csv'\n",
    "test_file = 'numerai_tournament_data.csv'\n",
    "lr_output_file = 'lr.csv'\n",
    "\n",
    "\n",
    "train = pd.read_csv( train_file )\n",
    "test = pd.read_csv( test_file )\n",
    "\n",
    "x_train = train.drop( ['target','p'], axis = 1 )\n",
    "y_train = train.target.values\n",
    "x_test = test.drop( 't_id', axis = 1 )\n",
    "val_size = len(test)\n",
    "\n",
    "val = train.iloc[-val_size:]\n",
    "y_val = val.target.values\n",
    "x_val = val.drop( ['target','p'], axis = 1 )\n",
    "\n",
    "pipe_lr = Pipeline([('lr', LR())])\n",
    "params_lr= dict(lr__C=[.7+.1*k for k in range(10)])\n",
    "grid_search = GridSearchCV(pipe_lr, param_grid=params_lr)\n",
    "grid_search.fit( x_train, y_train )\n",
    "y_lr=grid_search.predict_proba( x_test )[:,1]\n",
    "auc = AUC( y_val, y_lr )\n",
    "ll = log_loss( y_val, y_lr )\n",
    "print(\"AUC: {:.2%}, log loss: {:.2%} \\n\".format( auc, ll ))\n",
    "test['y_lr'] = y_lr\n",
    "test.to_csv( lr_output_file, columns = ( 't_id', 'y_lr' ), header = ( 't_id', 'probability' ), index = None )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How adding polynomial features will modify the result?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 50.38%, log loss: 69.55% \n",
      "\n",
      "AUC: 50.34%, log loss: 69.55% \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "poly_output_file = 'lr_poly.csv'\n",
    "poly_output_file_iO = 'lr_poly_interactionOnly.csv'\n",
    "\n",
    "lr_poly = make_pipeline( PolynomialFeatures(degree=2), LR()) \n",
    "lr_poly.fit( x_train, y_train )\n",
    "lr_poly_iO = make_pipeline( PolynomialFeatures(degree=2,interaction_only=True), LR()) \n",
    "lr_poly_iO.fit( x_train, y_train )\n",
    "\n",
    "y_poly = lr_poly.predict_proba( x_test )[:,1]\n",
    "auc = AUC( y_val, y_poly )\n",
    "ll = log_loss( y_val, y_poly )\n",
    "print(\"AUC: {:.2%}, log loss: {:.2%} \\n\".format( auc, ll ))\n",
    "\n",
    "y_poly_iO = lr_poly_iO.predict_proba( x_test )[:,1]\n",
    "auc = AUC( y_val, y_poly_iO )\n",
    "ll = log_loss( y_val, y_poly_iO )\n",
    "print(\"AUC: {:.2%}, log loss: {:.2%} \\n\".format( auc, ll ))\n",
    "\n",
    "test['y_poly'] = y_poly\n",
    "test['y_poly_iO'] = y_poly_iO\n",
    "test.to_csv( poly_output_file, columns = ( 't_id', 'y_poly' ), header = ( 't_id', 'probability' ), index = None )\n",
    "test.to_csv( poly_output_file_iO, columns = ( 't_id', 'y_poly_iO' ), header = ( 't_id', 'probability' ), index = None )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble learning classification methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 49.79%, log loss: 69.47% \n",
      "\n",
      "AUC: 50.15%, log loss: 69.49% \n",
      "\n",
      "AUC: 50.16%, log loss: 69.47% \n",
      "\n",
      "AUC: 50.13%, log loss: 69.33% \n",
      "\n",
      "AUC: 49.99%, log loss: 69.37% \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.random.seed(10)\n",
    "\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import (RandomTreesEmbedding, RandomForestClassifier,GradientBoostingClassifier)\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "n_estimator = 8\n",
    "# It is important to train the ensemble of trees on a different subset\n",
    "# of the training data than the linear regression model to avoid\n",
    "# overfitting, in particular if the total number of leaves is\n",
    "# similar to the number of training samples\n",
    "X_train, X_train_lr, Y_train, y_train_lr = train_test_split(x_train,\n",
    "                                                            y_train,\n",
    "                                                            test_size=0.5,random_state=int(2e+4))\n",
    "\n",
    "# Unsupervised transformation based on totally random trees\n",
    "rt = RandomTreesEmbedding(max_depth=3, n_estimators=n_estimator,\n",
    "    random_state=0)\n",
    "\n",
    "rt_lm = LogisticRegression()\n",
    "pipeline = make_pipeline(rt, rt_lm)\n",
    "pipeline.fit(X_train, Y_train)\n",
    "y_pred_rt = pipeline.predict_proba(x_test)[:, 1]\n",
    "auc = AUC( y_val, y_pred_rt )\n",
    "ll = log_loss( y_val, y_pred_rt )\n",
    "print(\"AUC: {:.2%}, log loss: {:.2%} \\n\".format( auc, ll ))\n",
    "test['y_rt'] = y_pred_rt\n",
    "test.to_csv( 'y_rt.csv', columns = ( 't_id', 'y_rt' ), header = ( 't_id', 'probability' ), index = None )\n",
    "\n",
    "# Supervised transformation based on random forests\n",
    "rf = RandomForestClassifier(max_depth=3, n_estimators=n_estimator)\n",
    "rf_enc = OneHotEncoder()\n",
    "rf_lm = LogisticRegression()\n",
    "rf.fit(X_train, Y_train)\n",
    "rf_enc.fit(rf.apply(X_train))\n",
    "rf_lm.fit(rf_enc.transform(rf.apply(X_train_lr)), y_train_lr)\n",
    "y_pred_rf_lm = rf_lm.predict_proba(rf_enc.transform(rf.apply(x_test)))[:, 1]\n",
    "auc = AUC( y_val, y_pred_rf_lm )\n",
    "ll = log_loss( y_val, y_pred_rf_lm )\n",
    "print(\"AUC: {:.2%}, log loss: {:.2%} \\n\".format( auc, ll ))\n",
    "test['y_rf+lr'] = y_pred_rf_lm\n",
    "test.to_csv( 'y_rf+lr.csv', columns = ( 't_id', 'y_rf+lr' ), header = ( 't_id', 'probability' ), index = None )\n",
    "\n",
    "\n",
    "grd = GradientBoostingClassifier(n_estimators=n_estimator)\n",
    "grd_enc = OneHotEncoder()\n",
    "grd_lm = LogisticRegression()\n",
    "grd.fit(X_train, Y_train)\n",
    "grd_enc.fit(grd.apply(X_train)[:, :, 0])\n",
    "grd_lm.fit(grd_enc.transform(grd.apply(X_train_lr)[:, :, 0]), y_train_lr)\n",
    "\n",
    "y_pred_grd_lm = grd_lm.predict_proba(grd_enc.transform(grd.apply(x_test)[:, :, 0]))[:, 1]\n",
    "auc = AUC( y_val, y_pred_grd_lm )\n",
    "ll = log_loss( y_val, y_pred_grd_lm )\n",
    "print(\"AUC: {:.2%}, log loss: {:.2%} \\n\".format( auc, ll ))\n",
    "test['y_gb+lr'] = y_pred_grd_lm\n",
    "test.to_csv( 'y_gb+lr.csv', columns = ( 't_id', 'y_gb+lr' ), header = ( 't_id', 'probability' ), index = None )\n",
    "\n",
    "# The gradient boosted model by itself\n",
    "y_pred_grd = grd.predict_proba(x_test)[:, 1]\n",
    "auc = AUC( y_val, y_pred_grd )\n",
    "ll = log_loss( y_val, y_pred_grd )\n",
    "print(\"AUC: {:.2%}, log loss: {:.2%} \\n\".format( auc, ll ))\n",
    "test['y_gb'] = y_pred_grd\n",
    "test.to_csv( 'y_gb.csv', columns = ( 't_id', 'y_gb' ), header = ( 't_id', 'probability' ), index = None )\n",
    "\n",
    "\n",
    "# The random forest model by itself\n",
    "y_pred_rf = rf.predict_proba(x_test)[:, 1]\n",
    "auc = AUC( y_val, y_pred_rf )\n",
    "ll = log_loss( y_val, y_pred_rf )\n",
    "print(\"AUC: {:.2%}, log loss: {:.2%} \\n\".format( auc, ll ))\n",
    "test['y_rf'] = y_pred_rf\n",
    "test.to_csv( 'y_rf.csv', columns = ( 't_id', 'y_rf' ), header = ( 't_id', 'probability' ), index = None )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA dimension reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 50.22%, log loss: 69.42% \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import decomposition\n",
    "\n",
    "\n",
    "pca = decomposition.PCA()\n",
    "lr=LR()\n",
    "pipe = Pipeline(steps=[('pca', pca), ('lr', lr)])\n",
    "\n",
    "n_components = [7, 14, 21]\n",
    "Cs = np.logspace(-4, 4, 3)\n",
    "\n",
    "#Parameters of pipelines can be set using ‘__’ separated parameter names:\n",
    "\n",
    "estimator = GridSearchCV(pipe,dict(pca__n_components=n_components,lr__C=Cs))\n",
    "estimator.fit(x_train, y_train)\n",
    "y_pca=estimator.predict_proba( x_test )[:,1]\n",
    "auc = AUC( y_val, y_pca )\n",
    "ll = log_loss( y_val, y_pca )\n",
    "print(\"AUC: {:.2%}, log loss: {:.2%} \\n\".format( auc, ll ))\n",
    "test['y_PCA'] = y_pca\n",
    "test.to_csv( 'y_PCA.csv', columns = ( 't_id', 'y_PCA' ), header = ( 't_id', 'probability' ), index = None )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
